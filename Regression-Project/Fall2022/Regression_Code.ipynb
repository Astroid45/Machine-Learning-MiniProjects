{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3cfd70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:25.746199Z",
     "start_time": "2024-04-03T21:24:25.604393Z"
    },
    "id": "cc3cfd70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from pdb import set_trace\n",
    "import sys\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1de4402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.056054Z",
     "start_time": "2024-04-03T21:24:25.748195Z"
    },
    "id": "e1de4402"
   },
   "outputs": [],
   "source": [
    "from util.timer import Timer\n",
    "from util.data import split_data, feature_label_split, Standardization\n",
    "from util.metrics import mse\n",
    "from datasets.HousingDataset import HousingDataset\n",
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MvWjqKVoHXts",
   "metadata": {
    "id": "MvWjqKVoHXts"
   },
   "source": [
    "Your Name: Xavier Rogers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab441f67",
   "metadata": {
    "id": "ab441f67"
   },
   "source": [
    "# Understand Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d66c2",
   "metadata": {
    "id": "400d66c2"
   },
   "source": [
    "![](https://assets.prevu.com/blogs/images/first-time-buyer-boston-real-estate/03d0c13cdf6721a022afd91e343493b5?ixlib=rb-4.0.3&w=670&lossless=true&auto=format%20compress&fit=fill&fill=solid&s=cb885d7fc811865d8d2219c47c87eb01)\n",
    "\n",
    "The dataset you'll be using for this project is the Boston Housing dataset which contains various different features about houses in Boston. This is a classic machine learning dataset from 1978 and is one of the first datasets most people use when first learning machine learning. **There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of the house, given by the 'MEDV' column, using the provided features.**\n",
    "\n",
    "The dataset consists of 3 splits:\n",
    "\n",
    "1. **Train**: Throughout this assignment you will be training your model using this data.\n",
    "2. **Validation**: You will then use this set to tune your model and evaluate its performance.\n",
    "3. **Test**: This split simulates real life data which we often don't have access to until the model is deployed. We have kept this split hidden from you and we will use it to judge the performance of your model.\n",
    "\n",
    "You DO NOT have access to the Test set as it gonna be used for scoring. This will not prevent you to complete this assignment at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595eb378",
   "metadata": {
    "id": "595eb378"
   },
   "source": [
    "The meanings of features and target are listed below. However, the meanings of these attributes will not affect your coding. So read them only if you are interested. \n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 USD\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n",
    "                 by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    14. MEDV     Median value of owner-occupied homes (in 1000 USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c2d05",
   "metadata": {
    "id": "046c2d05"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d55f7b9",
   "metadata": {
    "id": "6d55f7b9"
   },
   "source": [
    "# Design Machine Learning Models (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f1ec3",
   "metadata": {
    "id": "912f1ec3"
   },
   "source": [
    "## Base Model\n",
    "Basic model structure, **don't change** this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18e4417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.061546Z",
     "start_time": "2024-04-03T21:24:27.057882Z"
    },
    "id": "d18e4417"
   },
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5e9d1",
   "metadata": {
    "id": "ada5e9d1"
   },
   "source": [
    "## Linear Regression\n",
    "Basic model structure, **don't change** this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df00cf0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.067856Z",
     "start_time": "2024-04-03T21:24:27.062546Z"
    },
    "id": "df00cf0f"
   },
   "outputs": [],
   "source": [
    "class LinearModel(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for a linear model\n",
    "\n",
    "        Attributes\n",
    "        ==========\n",
    "        w       ndarray\n",
    "                weight vector/matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            weight vector w is initialized as None\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "\n",
    "    # check if the matrix is 2-dimensional. if not, raise an exception\n",
    "    def _check_matrix(self, mat, name):\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(f\"Your matrix {name} shape is not 2D! Matrix {name} has the shape {mat.shape}\")\n",
    "\n",
    "    # add a biases\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abstractmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "            train linear model\n",
    "\n",
    "            Args:\n",
    "                X:  Input data\n",
    "\n",
    "                y:  targets/labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "            apply the learned model to input X\n",
    "\n",
    "            parameters\n",
    "            ----------\n",
    "            X     2d array\n",
    "                  input data\n",
    "\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4babb8",
   "metadata": {
    "id": "7c4babb8"
   },
   "source": [
    "## TODO: Linear Regression with Ordinary Least Square (OLS) \n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92df3a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.074214Z",
     "start_time": "2024-04-03T21:24:27.069378Z"
    },
    "id": "92df3a11"
   },
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquares(LinearModel):\n",
    "    \"\"\"\n",
    "        Performs regression using ordinary least squares\n",
    "\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform OLS in order to learn the\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        self.w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        y_pred = X @ self.w\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = y_pred\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e361f50",
   "metadata": {
    "id": "8e361f50"
   },
   "source": [
    "## TODO: Lienar Regression with least Mean Squares (LMS)\n",
    "Optimize the model through gradient descent. **Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3becc614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.081196Z",
     "start_time": "2024-04-03T21:24:27.075213Z"
    },
    "id": "3becc614"
   },
   "outputs": [],
   "source": [
    "class LeastMeanSquares(LinearModel):\n",
    "    \"\"\"\n",
    "        Performs regression using least mean squares (gradient descent)\n",
    "\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix\n",
    "\n",
    "            alpha (float): learning rate or step size\n",
    "\n",
    "            epochs (int): Number of epochs to run for mini-batch\n",
    "                gradient descent\n",
    "\n",
    "            seed (int): Seed to be used for NumPy's RandomState class\n",
    "                or universal seed np.random.seed() function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float, epochs: int, seed: int = None):\n",
    "        super().__init__()\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform LMS in order to learn the\n",
    "                weights `self.w`.\n",
    "        \"\"\" \n",
    "        np.random.seed(self.seed)\n",
    "        _, features = X.shape\n",
    "        self.w = np.random.randn(features + 1)  \n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)  \n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                y_hat = np.dot(X[i], self.w)  \n",
    "                error = y_hat - y[i]  \n",
    "                self.w -= self.alpha * error * X[i]\n",
    "                \n",
    "          # TODO replace this line with your code\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        y_pred = X @ self.w\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = y_pred\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273151d9",
   "metadata": {
    "id": "273151d9"
   },
   "source": [
    "## TODO: Polynomial Regression with Ordinary Least Square (OLS) \n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962ee175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.088123Z",
     "start_time": "2024-04-03T21:24:27.082828Z"
    },
    "id": "962ee175"
   },
   "outputs": [],
   "source": [
    "class PolynomialRegression(OrdinaryLeastSquares):\n",
    "    \"\"\"\n",
    "        Performs polynomial regression using ordinary least squares algorithm\n",
    "\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix that is inherited from OrdinaryLeastSquares\n",
    "\n",
    "            degree (int): the number of polynomial degrees to include when adding\n",
    "                polynomial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree: int):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "\n",
    "    def add_polynomial_features(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes polynomial features given the pass data.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to compute the polynomial features\n",
    "                for X. Be sure to return the new data with the polynomial features!\n",
    "\n",
    "            Hint:\n",
    "                Feel free to use sklearn.preprocessing.PolynomialFeatures but remember\n",
    "                it includes the bias so make sure to disable said feature!\n",
    "        \"\"\"\n",
    "        poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "        return poly.fit_transform(X)\n",
    "          # TODO replace this line with your code\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform polynomial regression using\n",
    "                the closed form solution OLS to learn the weights `self.w`.\n",
    "\n",
    "            Hint:\n",
    "                Since we inherit from OrdinaryLeastSquares you can simply just call\n",
    "                super().train(X, y) instead of copying the code from OrdinaryLeastSquares\n",
    "                after you run self.add_polynomial_features(X).\n",
    "        \"\"\"\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        super().fit(X_poly, y)\n",
    "        # TODO replace this line with your code\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "\n",
    "            Hint:\n",
    "                Since we inherit from OrdinaryLeastSquares you can simply just call\n",
    "                super().predict(X) instead of copying the code from OrdinaryLeastSquares\n",
    "                after you run self.add_polynomial_features(X).\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        prediction = super().predict(X_poly)\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = prediction\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d18bb",
   "metadata": {
    "id": "a05d18bb"
   },
   "source": [
    "## TODO: Polynomial Regression with OLS and Regularization\n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ff8548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.095536Z",
     "start_time": "2024-04-03T21:24:27.089123Z"
    },
    "id": "d7ff8548"
   },
   "outputs": [],
   "source": [
    "class PolynomialRegressionRegularized(PolynomialRegression):\n",
    "    \"\"\"\n",
    "        Performs polynomial regression with l2 regularization using the ordinary least squares algorithm\n",
    "    \n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix that is inherited from OrdinaryLeastSquares\n",
    "            \n",
    "            degree (int): the number of polynomial degrees to include when adding\n",
    "                polynomial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree: int, lamb: float):\n",
    "        super().__init__(degree)\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "        \n",
    "            TODO:\n",
    "                Finish this method by adding code to perform polynomial regression using\n",
    "                the closed form solution OLS with L2 regularization to learn \n",
    "                the weights `self.w`.\n",
    "                \n",
    "            Hint:\n",
    "                Add the bias after computing polynomial features. Typically we don't want\n",
    "                to include the bias when computing polynomial features.\n",
    "        \"\"\"\n",
    "   \n",
    "        poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        n, m = X_poly.shape\n",
    "        X_poly = np.concatenate((np.ones((n, 1)), X_poly), axis=1)\n",
    "\n",
    "        n = X_poly.shape[1]\n",
    "        I = np.eye(n)\n",
    "        I[0,0] = 0\n",
    "        regularization_term = self.lamb * I\n",
    " \n",
    "        coefficients_matrix = np.linalg.pinv(X_poly.T @ X_poly + regularization_term)\n",
    "        self.w = coefficients_matrix @ X_poly.T @ y\n",
    "         # TODO replace this line with your code\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                This predict() method is exactly the same as the predict() method in the above class `PolynomialRegression`, \n",
    "                so you can just simply copy them here. \n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        X_poly = np.concatenate((np.ones((X_poly.shape[0], 1)), X_poly), axis=1)\n",
    "        prediction = X_poly @ self.w\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = prediction\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc97eb",
   "metadata": {
    "id": "08cc97eb"
   },
   "source": [
    "# TODO: Define Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7687cf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.102376Z",
     "start_time": "2024-04-03T21:24:27.095887Z"
    },
    "id": "7687cf22"
   },
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(name):\n",
    "        model = getattr(HyperParameters, name)\n",
    "        return {key:value for key, value in model.__dict__.items() \n",
    "            if not key.startswith('__') and not callable(key)}\n",
    "    \n",
    "    class OrdinaryLeastSquares():\n",
    "        pass # No hyperparamters to set\n",
    "        \n",
    "    class LeastMeanSquares():\n",
    "        model_kwargs = dict(\n",
    "            alpha = .0001, # TODO (REQUIRED) Set your learning rate\n",
    "            epochs = 100, # TODO (OPTIONAL) Set number of epochs\n",
    "            seed = 42, # TODO (OPTIONAL) Set seed for randomly generated weights\n",
    "        )\n",
    "\n",
    "        data_prep_kwargs = dict(\n",
    "            # TODO (OPTIONAL) Set the names of the features/columns to use for the Housing dataset\n",
    "            use_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
    "        )\n",
    "\n",
    "    class PolynomialRegression():\n",
    "        model_kwargs = dict(\n",
    "            degree = 3, # TODO (REQUIRED) Set your polynomial degree\n",
    "        )\n",
    "        \n",
    "    class PolynomialRegressionRegularized():\n",
    "        model_kwargs = dict(\n",
    "            degree = 3, # TODO (REQUIRED) Set your polynomial degree\n",
    "            lamb = 3, # TODO (REQUIRED) Set your regularization value for lambda\n",
    "        )\n",
    "\n",
    "        data_prep_kwargs = dict(\n",
    "            # TODO (OPTIONAL) Set the names of the features/columns to use for the Housing dataset\n",
    "            use_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe38b2",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd692453",
   "metadata": {
    "id": "bd692453"
   },
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4711cf4",
   "metadata": {
    "id": "c4711cf4"
   },
   "source": [
    "## Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f878e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.107456Z",
     "start_time": "2024-04-03T21:24:27.102892Z"
    },
    "id": "1f878e70"
   },
   "outputs": [],
   "source": [
    "def standardize_data(X_trn, X_vld):\n",
    "    standardize = Standardization()\n",
    "    X_trn_clean = standardize.fit_transform(X_trn)\n",
    "    X_eval_clean = standardize.transform(X_vld)\n",
    "    \n",
    "    return X_trn_clean, X_eval_clean\n",
    "\n",
    "def get_cleaned_data(df_trn, df_vld, feature_names, label_name, return_df=False):\n",
    "    X_trn, y_trn, X_vld, y_vld = split_data(df_trn, df_vld, feature_names, label_name)\n",
    "    X_trn, X_vld = standardize_data(X_trn, X_vld)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c266ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.113938Z",
     "start_time": "2024-04-03T21:24:27.109633Z"
    },
    "id": "97c266ec"
   },
   "outputs": [],
   "source": [
    "task_info = [\n",
    "       dict(\n",
    "            model=OrdinaryLeastSquares,\n",
    "            name='OrdinaryLeastSquares',\n",
    "            threshold=50,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=LeastMeanSquares,\n",
    "            name='LeastMeanSquares',\n",
    "            threshold=50,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=PolynomialRegression,\n",
    "            name='PolynomialRegression',\n",
    "            threshold=30,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=PolynomialRegressionRegularized,\n",
    "            name='PolynomialRegressionRegularized',\n",
    "            threshold=20,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951dc98",
   "metadata": {
    "id": "c951dc98"
   },
   "source": [
    "## Define Model running (training/fit and testing/evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6edccff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.119058Z",
     "start_time": "2024-04-03T21:24:27.115358Z"
    },
    "id": "b6edccff"
   },
   "outputs": [],
   "source": [
    "def get_name(obj):\n",
    "    try:\n",
    "        if hasattr(obj, '__name__'):\n",
    "            return obj.__name__\n",
    "        else:\n",
    "            return obj\n",
    "    except Exception as e:\n",
    "        return obj\n",
    "    \n",
    "def catch_and_throw(e, err):\n",
    "    trace = traceback.format_exc()\n",
    "    print(err + f\"\\n{trace}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657ddf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.129241Z",
     "start_time": "2024-04-03T21:24:27.120341Z"
    },
    "id": "657ddf50"
   },
   "outputs": [],
   "source": [
    "class RunModel():\n",
    "    t1 = '\\t'\n",
    "    t2 = '\\t\\t'\n",
    "    t3 = '\\t\\t\\t'\n",
    "    def __init__(self, model, model_params):\n",
    "        self.model_name = model.__name__\n",
    "        self.model_params = model_params\n",
    "        self.model = self.build_model(model, model_params)\n",
    "\n",
    "    def build_model(self, model, model_params):\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Building model {self.model_name}\")\n",
    "        \n",
    "        try:\n",
    "            model = model(**model_params)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while building model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        print(f\"{self.t1}Using hyperparameters: \")\n",
    "        [print(f\"{self.t2}{n} = {get_name(v)}\")for n, v in self.model_params.items()]\n",
    "        try: \n",
    "            return self._fit(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while training model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "    def _fit(self, X, y, metrics=None, pass_y=False):\n",
    "        if pass_y:\n",
    "            self.model.fit(X, y)\n",
    "        else:\n",
    "             self.model.fit(X)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix='Train')\n",
    "        return scores\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        print(f\"Evaluating {self.model_name}...\")\n",
    "        try:\n",
    "            return self._evaluate(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while evaluating model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        \n",
    "\n",
    "    def _evaluate(self, X, y, metrics, prefix=''):\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix)      \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            preds = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while making predictions for model {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "        return preds\n",
    "    \n",
    "    def get_metrics(self, y, y_hat, metrics, prefix=''):\n",
    "        scores = {}\n",
    "        for name, metric in metrics.items():\n",
    "            score = metric(y, y_hat)\n",
    "            display_score = round(score, 3)\n",
    "            scores[name] = score\n",
    "            print(f\"{self.t2}{prefix} {name}: {display_score}\")\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb24c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.139106Z",
     "start_time": "2024-04-03T21:24:27.131150Z"
    },
    "id": "edb24c7b"
   },
   "outputs": [],
   "source": [
    "def run_eval(eval_stage='validation',task_infos=task_info):\n",
    "    main_timer = Timer()\n",
    "    main_timer.start()\n",
    "\n",
    "    dataset = HousingDataset()\n",
    "    df_trn, df_vld = dataset.load()\n",
    "\n",
    "    total_points = 0\n",
    "      \n",
    "    for info in task_infos:\n",
    "        task_timer =  Timer()\n",
    "        task_timer.start()\n",
    "        try: \n",
    "            params = HyperParameters.get_params(info['name'])\n",
    "            model_kwargs = params.get('model_kwargs', {})\n",
    "            data_prep_kwargs = params.get('data_prep_kwargs', {})\n",
    "\n",
    "            if info['name'] == 'OrdinaryLeastSquares':\n",
    "                feature_names = \"RM\"\n",
    "            elif info['name'] == 'PolynomialRegression':\n",
    "                feature_names = \"LSTAT\" \n",
    "            else:\n",
    "                use_features = data_prep_kwargs.get('use_features')\n",
    "                if use_features is None:\n",
    "                    err = f\"use_features argument for {info['name']} can not be none: received {use_features}\"\n",
    "                    raise ValueError(err)\n",
    "                elif  len(use_features) < 2 :\n",
    "                    err = f\"use_features argument for {info['name']} must have at least 2 features: received {use_features}\"\n",
    "                    raise ValueError(err)\n",
    "                \n",
    "                feature_names = data_prep_kwargs['use_features']\n",
    "\n",
    "            run_model = RunModel(info['model'], model_kwargs)\n",
    "\n",
    "            \n",
    "            X_trn, y_trn, X_vld, y_vld = get_cleaned_data(df_trn, df_vld, feature_names, \"MEDV\")\n",
    "\n",
    "            trn_scores = run_model.fit(X_trn, y_trn, info['metrics'], pass_y=True)  # Model training\n",
    "            eval_scores = run_model.evaluate(X_vld, y_vld, info['metrics'], prefix=eval_stage.capitalize()) # Model testing\n",
    "\n",
    "            info['trn_score'] = trn_scores[info['eval_metric']]\n",
    "            info['eval_score'] = eval_scores[info['eval_metric']]\n",
    "            info['successful'] = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            track = traceback.format_exc()\n",
    "            print(\"The following exception occurred while executing this test case:\\n\", track)\n",
    "        task_timer.stop()\n",
    "        \n",
    "        print(\"\")\n",
    "        points = rubric_regression(info['eval_score'], info['threshold'])\n",
    "        print(f\"Points Earned: {points}\")\n",
    "        total_points += points\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print('')\n",
    "    main_timer.stop()\n",
    "\n",
    "    avg_trn_mse, avg_eval_mse, successful_tests = summary(task_info)\n",
    "    task_eval_mse = get_eval_scores(task_info)\n",
    "    total_points = int(round(total_points))\n",
    "\n",
    "    print(f\"MSE averages for {successful_tests} successful tests\")\n",
    "    print(f\"\\tAverage Train MSE: {avg_trn_mse}\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"\\tAverage {eval_stage.capitalize()} MSE: {avg_eval_mse}\")\n",
    "    \n",
    "    return (total_points, avg_trn_mse, avg_eval_mse, *task_eval_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a68e4",
   "metadata": {
    "id": "391a68e4"
   },
   "source": [
    "## Evaluation Related Functions\n",
    "Don't change this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d653ca94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.145238Z",
     "start_time": "2024-04-03T21:24:27.140164Z"
    },
    "id": "d653ca94"
   },
   "outputs": [],
   "source": [
    "def rubric_regression(mse, thresh, max_score=20):\n",
    "    if mse <= thresh:\n",
    "        score_percent = 100\n",
    "    elif mse is not None:\n",
    "        score_percent = (thresh / mse) * 100\n",
    "        if score_percent < 40:\n",
    "            score_percent = 40\n",
    "    else:\n",
    "        score_percent = 20\n",
    "    score = max_score * score_percent / 100.0\n",
    "\n",
    "    return score\n",
    "\n",
    "def get_eval_scores(task_info):\n",
    "    return [i['eval_score'] for i in task_info]\n",
    "\n",
    "def summary(task_info):\n",
    "    sum_trn_mse = 0\n",
    "    sum_eval_mse = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    for info in task_info:\n",
    "        if info['successful']:\n",
    "            successful_tests += 1\n",
    "            sum_trn_mse += info['trn_score']\n",
    "            sum_eval_mse += info['eval_score']\n",
    "    \n",
    "    if successful_tests == 0:\n",
    "        return 9999, 9999, successful_tests\n",
    "    \n",
    "    avg_trn_mse = sum_trn_mse / successful_tests\n",
    "    avg_eval_mse = sum_eval_mse / successful_tests\n",
    "    return avg_trn_mse, avg_eval_mse, successful_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c0ef0",
   "metadata": {
    "id": "ad7c0ef0"
   },
   "source": [
    "# Test your code\n",
    "Run the following cell to test your code (or for **debugging**). Currently, the last line of output is \"Average Validation MSE: 548.01\". The number is super high because the TODOs are still empty (no machine learning model is used) right now. **Please fill in the TODOs through this ipython file and try your best to get a low MSE.** The \"Average Validation MSE\" will decide your score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c6bd3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T21:24:27.440697Z",
     "start_time": "2024-04-03T21:24:27.146651Z"
    },
    "id": "58c6bd3b",
    "outputId": "f8437391-82f9-41be-92f7-13a4686d1082",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download. File already exists: C:\\Users\\zayma\\Fall2022\\Fall2022\\datasets\\data\\housing.train\n",
      "\n",
      "Skipping download. File already exists: C:\\Users\\zayma\\Fall2022\\Fall2022\\datasets\\data\\housing.val\n",
      "\n",
      "Skipping download. File already exists: C:\\Users\\zayma\\Fall2022\\Fall2022\\datasets\\data\\housing.names\n",
      "\n",
      "==================================================\n",
      "Building model OrdinaryLeastSquares\n",
      "Training OrdinaryLeastSquares...\n",
      "\tUsing hyperparameters: \n",
      "\t\tTrain MSE: 41.941\n",
      "Evaluating OrdinaryLeastSquares...\n",
      "\t\tValidation MSE: 47.325\n",
      "Elapsed time: 0.0214 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model LeastMeanSquares\n",
      "Training LeastMeanSquares...\n",
      "\tUsing hyperparameters: \n",
      "\t\talpha = 0.0001\n",
      "\t\tepochs = 100\n",
      "\t\tseed = 42\n",
      "\t\tTrain MSE: 22.95\n",
      "Evaluating LeastMeanSquares...\n",
      "\t\tValidation MSE: 24.817\n",
      "Elapsed time: 0.1960 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model PolynomialRegression\n",
      "Training PolynomialRegression...\n",
      "\tUsing hyperparameters: \n",
      "\t\tdegree = 3\n",
      "\t\tTrain MSE: 31.826\n",
      "Evaluating PolynomialRegression...\n",
      "\t\tValidation MSE: 21.925\n",
      "Elapsed time: 0.0021 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model PolynomialRegressionRegularized\n",
      "Training PolynomialRegressionRegularized...\n",
      "\tUsing hyperparameters: \n",
      "\t\tdegree = 3\n",
      "\t\tlamb = 3\n",
      "\t\tTrain MSE: 1.498\n",
      "Evaluating PolynomialRegressionRegularized...\n",
      "\t\tValidation MSE: 15.901\n",
      "Elapsed time: 0.0619 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "\n",
      "Elapsed time: 0.2910 seconds\n",
      "MSE averages for 4 successful tests\n",
      "\tAverage Train MSE: 24.553517832457175\n",
      "=======================================================\n",
      "\tAverage Validation MSE: 27.491982959110096\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_eval()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e69b7",
   "metadata": {
    "id": "944e69b7"
   },
   "source": [
    "# Easy Points (20 points)\n",
    "In case the above coding is too difficult to you guys, here are simple questions which will counts for 20 points (the above coding counts for 80 points) of this assignment. Answer the questions based your understanding of the dataset. Each question worths 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a596435",
   "metadata": {
    "id": "5a596435"
   },
   "source": [
    "Q1: In training dataset, there are how many samples?\n",
    "\n",
    "A1:In the training dataset, there are 506 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3d932",
   "metadata": {
    "id": "bde3d932"
   },
   "source": [
    "Q2: In validation dataset, there are how many samples?\n",
    "\n",
    "A2: There are 506 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8cfc",
   "metadata": {
    "id": "f90d8cfc"
   },
   "source": [
    "Q3: In each sample, there are how many input features (or variable)?\n",
    "\n",
    "A3: There are 13 input features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fd19e",
   "metadata": {
    "id": "6b7fd19e"
   },
   "source": [
    "Q4: What's the output/target variable (predict variable)? Please answer the column name of the target variable. Don't include quote mark. \n",
    "\n",
    "A4: The output/target variable (predict variable) column name is MEDV. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb7921",
   "metadata": {
    "id": "71fb7921"
   },
   "source": [
    "Q5: What are the input variable (or input features)? Please answer the column names of the input features. Don't include quote mark. \n",
    "\n",
    "A5: The input variables (input features) consist of columns representing attributes CRIM, ZN, INDUS, CHAS, NOX, RM, AGE,DIS, RAD, TAX, PTRATIO, B, LSTAT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7fe9",
   "metadata": {
    "id": "2c9c7fe9"
   },
   "source": [
    "Q6: In training datset, what's the mean value of `RM` (Keep two decimal places)? \n",
    "\n",
    "A6: RM = 6.60. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6ae87",
   "metadata": {
    "id": "7ba6ae87"
   },
   "source": [
    "Q7: **After standarldization**, what's the value of `AGE` variable of the 1st sample (the 0-th sample) in training set (Keep two decimal places)? \n",
    "\n",
    "A7: AGE =  20.92. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56988b75",
   "metadata": {
    "id": "56988b75"
   },
   "source": [
    "Q8: The output/target variable is 'Continuous' or 'Discrete'?\n",
    "\n",
    "A8: The output/target variable is Continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a772df",
   "metadata": {
    "id": "46a772df"
   },
   "source": [
    "Q9: Will the regression with Ordinary Least Square based on gradient descent or not? Please answer YES or NO.\n",
    "\n",
    "A9: NO. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a949279",
   "metadata": {
    "id": "0a949279"
   },
   "source": [
    "Q10: In Regularization, increase the coefficient lambda, the model will be more 'Simple' or 'Complex'? \n",
    "\n",
    "A10: Increasing the coefficient lambda in regularization typically leads to a simpler model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6bdbd",
   "metadata": {
    "id": "aac6bdbd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
